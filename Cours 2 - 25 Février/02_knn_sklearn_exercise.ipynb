{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-NN with sklearn\n",
    "\n",
    "This notebook was written by Gael Lederrey and Tim Hillel (tim.hillel@epfl.ch) for the Decision-aid methodologies in transportation course at EPFL (http://edu.epfl.ch/coursebook/en/decision-aid-methodologies-in-transportation-CIVIL-557).\n",
    "\n",
    "Please contact before distributing or reusing the material below.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Now that we've implemented the k-NN algorithm, we will see how to use it with the `scikit-learn` library. In this notebook, we will learn to:\n",
    "\n",
    "1. Scale data using scikit-learn\n",
    "2. Use classifiers from scikit-learn\n",
    "3. Test different model hyperparameters\n",
    "4. Use different metrics to assess the performance of your model\n",
    "\n",
    "\n",
    "## Set-up\n",
    "\n",
    "We start by loading the dataset and the different libraries that are required for the exercices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv('data/dataset.csv')\n",
    "\n",
    "# We subsample the dataset to reduce the computational cost\n",
    "df = df_full.sample(10000, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the multinomial version of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary used to transform the string in \n",
    "# the travel_mode to an integer\n",
    "str_to_val = {\n",
    "    'walk': 0,\n",
    "    'cycle': 1,\n",
    "    'pt': 2,\n",
    "    'drive': 3\n",
    "}\n",
    "\n",
    "# Output\n",
    "y = df['travel_mode'].replace(str_to_val).values\n",
    "\n",
    "# Features (4 are selected)\n",
    "x = df[['age', 'car_ownership', 'distance', 'female']].values\n",
    "\n",
    "# We split the output and features into a train and a test set by \n",
    "# an (approximate) ratio of 0.8\n",
    "np.random.seed(123)\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "\n",
    "x_train_unscaled = x[msk]\n",
    "x_test_unscaled = x[~msk]\n",
    "\n",
    "y_train = y[msk]\n",
    "y_test = y[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN with scikit-learn\n",
    "\n",
    "In this section, we will use the $k$-NN from ``sklearn`` and compare the results to our implementation in the previous notebook.\n",
    "\n",
    "The `KNeighborsClassifier` is in the ``neighbors`` submodule of sklearn. Try importing it directly.\n",
    "\n",
    "Notice the *CamelCase*? This tells us we are using a class! (Like `DataFrame` in Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn classes all behave in a very simular way (including classifiers, regressers, scalers, etc.)\n",
    "\n",
    "Firstly, we *instantiate* the class (i.e. create an instance). Try using the *help* functionality to investigate the hyperparameters and default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we `fit` the class to the training data. (Note, this doesn't actually do anything for $k$-NN, as our model is simply the data!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(x_train_unscaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the class on new data. For classifiers, we use them to **predict** new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(x_test_unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our accuracy score function again, and use it to compare the results of the skleanr model to our implimentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    return np.mean(np.array(y_true)==np.array(y_pred))*100\n",
    "\n",
    "print(\"Accuracy: {:.3f}%\".format(compute_accuracy(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we get exactly the same score (for the multinomial case)!\n",
    "\n",
    "This is because we used the same *hyperparameters*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling data\n",
    "\n",
    "As discussed, $k$-NN is highly sensitive to data scaling. As such, we will use the standard scaler from scikit-learn to scale the data to zero-mean unit-variance.\n",
    "\n",
    "The `StandardScaler` is in the preprocessing submodule of sklearn. Try importing it directly, instantiating it, and fitting it to `x_train_unscaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of `predict`, we use the scaler to `transform` data.\n",
    "\n",
    "Use the fitted scaler to transform `x_train_unscaled` and `x_test_unscaled` and save it as `x_train` and `x_test` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try using the `knn` to the scaled data, and see how the results have changed!\n",
    "\n",
    "You should get an accuracy score of 65.457%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy, Precision, and Recall\n",
    "\n",
    "As discussed, accuracy is not always the best policy. \n",
    "\n",
    "Let's investigate some other metric, based on the `confusion_matrix`\n",
    "\n",
    "Firstly, import the function `confusion_matrix` from `metrics` submodule of `sklearn`, and display the confusion matrix for the predicted values `y_pred` and ground truth values `y_test` for the test data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall that (for a single class):\n",
    "\n",
    "`precision = TP/TP+FP\n",
    "recall = TP/TP+FN`\n",
    "\n",
    "Write functions to compute the precision and recall for a given class, from the confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision(y_true, y_pred, c):\n",
    "    # Enter your code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall(y_true, y_pred, c):\n",
    "    # Enter your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try printing the precision and recall for the `pt` mode. You should get 0.685 and 0.669 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a list comprehension to get the precision and recall for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function `precision_recall_fscore_support` to verify our answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "prec, rec, fscore, supp = precision_recall_fscore_support(y_test, y_pred)\n",
    "\n",
    "print(\"Precision: {}\".format(prec))\n",
    "print(\"Recall: {}\".format(rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model optimisation\n",
    "\n",
    "We can investigate the effects of the hyperparameters on model performance. \n",
    "\n",
    "### $k$\n",
    "\n",
    "Let's try investigating the effect `k` on the model. Try testing mutliple values of `k` (e.g. between 1 and 50) and comment on the results. \n",
    "\n",
    "For now, use accuracy as the performance metric to focus on(!)\n",
    "\n",
    "*Hint*: It could be useful to plot a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was your best value of $k$? Is it the same as others in the class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Bonus) Repeated trials\n",
    "\n",
    "In order to to estimate the confidence interfals for our performance estimates, we could test the multiple on multiple draws from the dataset, *i.e.* draw the train and test sets multiple times. \n",
    "\n",
    "*Note, there are other ways to do this, which we will discuss later in the course*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Bonus) Other hyperparameters \n",
    "\n",
    "Try experimenting with the other hyperparameters in $k$-NN (use the documentation!). \n",
    "\n",
    "E.g. what happens if we use distance based weightings? Does the optimal value of $k$ change?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
